{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, Dot, Multiply, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Download and preprocess the CNN/Daily Mail dataset\n",
        "dataset_url = 'http://cs.nyu.edu/~kcho/DMQA/'\n",
        "train_url = os.path.join(dataset_url, 'dm_train_tokenized.npy')\n",
        "val_url = os.path.join(dataset_url, 'dm_validation_tokenized.npy')\n",
        "test_url = os.path.join(dataset_url, 'dm_test_tokenized.npy')\n",
        "\n",
        "train_data = np.load(train_url, allow_pickle=True)\n",
        "val_data = np.load(val_url, allow_pickle=True)\n",
        "test_data = np.load(test_url, allow_pickle=True)\n",
        "\n",
        "# Define the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data[:, 0])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Define the maximum sequence length\n",
        "max_sequence_length = 500\n",
        "\n",
        "# Pad the sequences\n",
        "train_data_x = pad_sequences(tokenizer.texts_to_sequences(train_data[:, 0]), maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "train_data_y = pad_sequences(tokenizer.texts_to_sequences(train_data[:, 1]), maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "val_data_x = pad_sequences(tokenizer.texts_to_sequences(val_data[:, 0]), maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "val_data_y = pad_sequences(tokenizer.texts_to_sequences(val_data[:, 1]), maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "test_data_x = pad_sequences(tokenizer.texts_to_sequences(test_data[:, 0]), maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_data_y = pad_sequences(tokenizer.texts_to_sequences(test_data[:, 1]), maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Define the hyperparameters\n",
        "embed_dim = 300\n",
        "hidden_dim = 512\n",
        "num_layers = 2\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.0002\n",
        "\n",
        "# Define the discriminator model\n",
        "def build_discriminator_model(vocab_size, embed_dim, hidden_dim, max_sequence_length):\n",
        "    input_x = Input(shape=(max_sequence_length,))\n",
        "    embedding_x = Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=max_sequence_length)(input_x)\n",
        "    lstm_x = LSTM(units=hidden_dim, return_sequences=True)(embedding_x)\n",
        "    dense_x = Dense(units=1, activation='sigmoid')(lstm_x)\n",
        "    model = Model(inputs=input_x, outputs=dense_x)\n",
        "    return model\n",
        "\n",
        "# Define the generator model\n",
        "def build_generator_model(vocab_size, embed_dim, hidden_dim, num_layers, max_sequence_length):\n",
        "    input_z = Input(shape=(max_sequence_length,))\n",
        "    embedding_z = Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=max_sequence_length)(input_z)\n",
        "    lstm_z = LSTM(units=hidden_dim, return_sequences=True)(embedding_z)\n",
        "    for i in range(num_layers-1):\n",
        "        lstm_z = LSTM(units=hidden_dim, return_sequences=True)(lstm_z)\n",
        "    output_z = Dense(units=vocab_size, activation='softmax')(lstm_z)\n",
        "    model = Model(inputs=input_z, outputs=output_z)\n",
        "    return model\n",
        "\n",
        "# Define the feature alignment discriminator model\n",
        "class FADiscriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(FADiscriminator, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, 1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(self.relu(self.linear1(x)))\n",
        "        x = self.dropout(self.relu(self.linear2(x)))\n",
        "        x = torch.sigmoid(self.linear3(x))\n",
        "        return x\n",
        "\n",
        "# Define the feature alignment discriminator optimizer\n",
        "fa_optimizer = optim.Adam(fa_discriminator.parameters(), lr=lr)\n",
        "\n",
        "# Define the feature alignment discriminator loss\n",
        "fa_criterion = nn.BCELoss()\n",
        "\n",
        "# Train the feature alignment discriminator\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    epoch_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Generate summaries\n",
        "        outputs, scores = model(inputs)\n",
        "\n",
        "        # Compute feature alignment loss\n",
        "        feature_alignment_loss = criterion(scores, targets)\n",
        "\n",
        "        # Train the feature alignment discriminator\n",
        "        fa_optimizer.zero_grad()\n",
        "\n",
        "        # Positive samples\n",
        "        pos_inputs = scores.detach().clone()\n",
        "        pos_targets = torch.ones(pos_inputs.shape[0], 1).to(device)\n",
        "        pos_outputs = fa_discriminator(pos_inputs)\n",
        "\n",
        "        # Negative samples\n",
        "        neg_inputs = targets.detach().clone()\n",
        "        neg_targets = torch.zeros(neg_inputs.shape[0], 1).to(device)\n",
        "        neg_outputs = fa_discriminator(neg_inputs)\n",
        "\n",
        "        # Compute feature alignment discriminator loss\n",
        "        fa_loss = fa_criterion(pos_outputs, pos_targets) + fa_criterion(neg_outputs, neg_targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        fa_loss.backward()\n",
        "        fa_optimizer.step()\n",
        "\n",
        "        # Compute reconstruction loss\n",
        "        reconstruction_loss = criterion(outputs, targets[:, 1:])\n",
        "\n",
        "        # Compute total loss\n",
        "        loss = feature_alignment_loss + reconstruction_loss - fa_loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch, n_epochs, epoch_loss / len(train_loader)))\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Generate summaries\n",
        "        outputs, _ = model(inputs)\n",
        "\n",
        "        # Print input and output summaries\n",
        "        print('Input summary: ', tokenizer.decode(inputs[0].tolist(), skip_special_tokens=True))\n",
        "        print('Output summary: ', tokenizer.decode(outputs[0].tolist(), skip_special_tokens=True))\n",
        "        print('Target summary: ', tokenizer.decode(targets[0].tolist(), skip_special_tokens=True))\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "Vcz8xhM41qZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}